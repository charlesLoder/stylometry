{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aae31535",
   "metadata": {},
   "source": [
    "# Generating a style profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d4731a",
   "metadata": {},
   "source": [
    "Another strategy for controlling model style is to build a prompt based off of the stylometric profile of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b27e20",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7268b677",
   "metadata": {},
   "source": [
    "We will the same data from [before](./01-generating-a-dataset.ipynb).\n",
    "All the manifests should be cached by `loam-iiif`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a495cd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Found 5000 manifests\n",
      "âœ… Processed 4906 manifests\n"
     ]
    }
   ],
   "source": [
    "from loam_iiif import iiif\n",
    "\n",
    "client = iiif.IIIFClient()\n",
    "\n",
    "# Berkeley Folk Music Festival\n",
    "collection_url = \"https://api.dc.library.northwestern.edu/api/v2/collections/18ec4c6b-192a-4ab8-9903-ea0f393c35f7?as=iiif\"\n",
    "max_manifests = 5000\n",
    "\n",
    "manifest_ids, _collection_ids = client.get_manifests_and_collections_ids(collection_url, max_manifests)\n",
    "\n",
    "print(f\"ðŸ”Ž Found {len(manifest_ids)} manifests\")\n",
    "\n",
    "data: list[str] = []\n",
    "\n",
    "for id in manifest_ids:\n",
    "    manifest = client.fetch_json(id)\n",
    "\n",
    "    if \"summary\" not in manifest:\n",
    "        continue\n",
    "\n",
    "    summary: dict = manifest[\"summary\"]\n",
    "    keys = summary.keys()\n",
    "\n",
    "    if len(keys) == 0:\n",
    "        continue\n",
    "\n",
    "    summary_text: str = \"\"\n",
    "    if \"none\" in keys:\n",
    "        summary_text = \"\\n\".join(summary[\"none\"])\n",
    "    else:\n",
    "        summary_text = \"\\n\".join(summary[keys[0]])\n",
    "\n",
    "    data.append(summary_text)\n",
    "\n",
    "print(f\"âœ… Processed {len(data)} manifests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec813c35",
   "metadata": {},
   "source": [
    "## Stylometric profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a2c8c9",
   "metadata": {},
   "source": [
    "Next, we create a profile of the summaries based off of the field of [stylometry](https://en.wikipedia.org/wiki/Stylometry).\n",
    "This is a very rudimentary analysis, but it will serve well enough for an LLM prompt.\n",
    "\n",
    "We will not dive into the theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35f8ad56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/charles/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/charles/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/charles/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Download required NLTK data (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "def basic_stylometric_profile(texts: list[str]):\n",
    "    \"\"\"\n",
    "    Generates a basic stylometric profile for a list of texts.\n",
    "\n",
    "    Args:\n",
    "        texts: A list of strings (each string is a document).\n",
    "\n",
    "    Returns:\n",
    "        A Pandas DataFrame containing the stylometric features for each text.\n",
    "    \"\"\"\n",
    "\n",
    "    num_texts = len(texts)\n",
    "    results = {}\n",
    "\n",
    "    results['num_words'] = [len(text.split()) for text in texts]\n",
    "\n",
    "    results['avg_word_length'] = [np.mean([len(word) for word in text.split()]) for text in texts]\n",
    "\n",
    "    results['num_sentences'] = [len(sent_tokenize(text)) for text in texts] # Use nltk.sent_tokenize\n",
    "\n",
    "    results['avg_sentence_length'] = [results['num_words'][i] / results['num_sentences'][i] if results['num_sentences'][i] > 0 else 0 for i in range(num_texts)]\n",
    "\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    results['stopword_frequency'] = [sum([1 for word in texts[i].lower().split() if word in stop_words]) / results['num_words'][i] if results['num_words'][i] > 0 else 0 for i in range(num_texts)]\n",
    "\n",
    "    results['comma_frequency'] = [text.count(',') / len(text) if len(text) > 0 else 0 for text in texts]\n",
    "\n",
    "    def type_token_ratio(text):\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        types = set(tokens)\n",
    "        return len(types) / len(tokens) if len(tokens) > 0 else 0\n",
    "\n",
    "    results['type_token_ratio'] = [type_token_ratio(text) for text in texts]\n",
    "\n",
    "    # Punctuation Frequency\n",
    "    def punctuation_frequency(text):\n",
    "        punctuation = \"!\\\"#$%&'()*+,-./:;<=>?@[]^_`{|}~\"\n",
    "        counts = Counter(c for c in text if c in punctuation)\n",
    "        total_punctuation = sum(counts.values())\n",
    "        return total_punctuation / len(text) if len(text) > 0 else 0\n",
    "\n",
    "    results['punctuation_frequency'] = [punctuation_frequency(text) for text in texts]\n",
    "\n",
    "    # Passive Voice Ratio (approximate, requires more sophisticated parsing for perfect accuracy)\n",
    "    def passive_voice_ratio(text):\n",
    "        sentences = sent_tokenize(text)\n",
    "        passive_count = 0\n",
    "        for sentence in sentences:\n",
    "            tokens = word_tokenize(sentence)\n",
    "            tagged = nltk.pos_tag(tokens)\n",
    "            for i in range(len(tagged) - 1):\n",
    "                if tagged[i][1] == 'VBN' and (tagged[i+1][0].lower() == 'by' or tagged[i-1][0].lower() in ['is', 'are', 'was', 'were', 'be', 'been', 'being']):  #Basic heuristic\n",
    "                    passive_count += 1\n",
    "                    break # Count only one passive per sentence.\n",
    "        return passive_count / len(sentences) if sentences else 0\n",
    "\n",
    "\n",
    "    results['passive_voice_ratio'] = [passive_voice_ratio(text) for text in texts]\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "# Generate the stylometric profile\n",
    "profile_df = basic_stylometric_profile(data)\n",
    "\n",
    "# Calculate statistics for sentence length\n",
    "mean_sentence_length = profile_df['avg_sentence_length'].mean()\n",
    "std_sentence_length = profile_df['avg_sentence_length'].std()\n",
    "lower_bound = int(round(mean_sentence_length - std_sentence_length))\n",
    "upper_bound = int(round(mean_sentence_length + std_sentence_length))\n",
    "lower_bound = max(1, lower_bound)  # Ensure it's not negative\n",
    "upper_bound = max(lower_bound + 1, upper_bound) # Ensure upper bound > lower bound\n",
    "\n",
    "# Calculate stop word frequency\n",
    "mean_stopword_frequency = profile_df['stopword_frequency'].mean()\n",
    "\n",
    "# Calculate comma frequency\n",
    "mean_comma_frequency = profile_df['comma_frequency'].mean()\n",
    "std_comma_frequency = profile_df['comma_frequency'].std()\n",
    "comma_lower_bound = max(0, mean_comma_frequency - std_comma_frequency)\n",
    "comma_upper_bound = mean_comma_frequency + std_comma_frequency\n",
    "\n",
    "# Calculate passive voice ratio\n",
    "mean_passive_voice_ratio = profile_df['passive_voice_ratio'].mean()\n",
    "std_passive_voice_ratio = profile_df['passive_voice_ratio'].std()\n",
    "passive_voice_lower_bound = max(0, mean_passive_voice_ratio - std_passive_voice_ratio)\n",
    "passive_voice_upper_bound = mean_passive_voice_ratio + std_passive_voice_ratio\n",
    "\n",
    "\n",
    "def check_sentence_length(sentence, lower_bound, upper_bound):\n",
    "    num_words = len(sentence.split())\n",
    "    return lower_bound <= num_words <= upper_bound\n",
    "\n",
    "\n",
    "def check_stopword_frequency(sentence, mean_stopword_frequency):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    num_words = len(sentence.split())\n",
    "    stopword_count = sum(1 for word in sentence.lower().split() if word in stop_words)\n",
    "    stopword_frequency = stopword_count / num_words if num_words > 0 else 0\n",
    "    return abs(stopword_frequency - mean_stopword_frequency) <= 0.1 # Tolerance\n",
    "\n",
    "\n",
    "def check_comma_frequency(sentence, comma_lower_bound, comma_upper_bound):\n",
    "    comma_frequency = sentence.count(',') / len(sentence) if len(sentence) > 0 else 0\n",
    "    return comma_lower_bound <= comma_frequency <= comma_upper_bound\n",
    "\n",
    "\n",
    "def check_passive_voice_ratio(sentence, mean_passive_voice_ratio):\n",
    "     tokens = word_tokenize(sentence)\n",
    "     tagged = nltk.pos_tag(tokens)\n",
    "     passive_count = 0\n",
    "     for i in range(len(tagged) - 1):\n",
    "         if tagged[i][1] == 'VBN' and (tagged[i+1][0].lower() == 'by' or tagged[i-1][0].lower() in ['is', 'are', 'was', 'were', 'be', 'been', 'being']):  #Basic heuristic\n",
    "             passive_count += 1\n",
    "             break # Count only one passive per sentence.\n",
    "     ratio = passive_count / len(sent_tokenize(sentence)) if len(sent_tokenize(sentence)) > 0 else 0\n",
    "\n",
    "     return abs(ratio - mean_passive_voice_ratio) <= 0.1 #Tolerance\n",
    "\n",
    "\n",
    "def find_example_sentences(texts, num_examples=2,\n",
    "                           lower_bound=None, upper_bound=None,\n",
    "                           mean_stopword_frequency=None,\n",
    "                           comma_lower_bound=None, comma_upper_bound=None,\n",
    "                           mean_passive_voice_ratio = None):\n",
    "    \"\"\"Finds sentences that match specified stylistic properties.  Properties that aren't specified are ignored\"\"\"\n",
    "\n",
    "    examples = []\n",
    "    possible_examples = []\n",
    "\n",
    "    for text in texts:\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            # Check all relevant criteria\n",
    "            length_ok = True if lower_bound is None else check_sentence_length(sentence, lower_bound, upper_bound)\n",
    "            stopword_ok = True if mean_stopword_frequency is None else check_stopword_frequency(sentence, mean_stopword_frequency)\n",
    "            comma_ok = True if comma_lower_bound is None else check_comma_frequency(sentence, comma_lower_bound, comma_upper_bound)\n",
    "            passive_voice_ok = True if mean_passive_voice_ratio is None else check_passive_voice_ratio(sentence, mean_passive_voice_ratio)\n",
    "\n",
    "            if length_ok and stopword_ok and comma_ok and passive_voice_ok:\n",
    "                possible_examples.append(sentence)\n",
    "\n",
    "    if len(possible_examples) >= num_examples:\n",
    "        examples = random.sample(possible_examples, num_examples)\n",
    "    else:\n",
    "        examples = random.choices(possible_examples, k=num_examples) # Allow duplicates when the number of examples is low\n",
    "\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "example_sentence_lengths = find_example_sentences(data, num_examples=2, lower_bound=lower_bound, upper_bound=upper_bound)\n",
    "example_stopword_frequencies = find_example_sentences(data, num_examples=2, mean_stopword_frequency=mean_stopword_frequency)\n",
    "example_comma_frequencies = find_example_sentences(data, num_examples=2, comma_lower_bound=comma_lower_bound, comma_upper_bound=comma_upper_bound)\n",
    "example_passive_voice_ratios = find_example_sentences(data, num_examples=2, mean_passive_voice_ratio = mean_passive_voice_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463193a4",
   "metadata": {},
   "source": [
    "After doing the calculations, we'll create a prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd47a80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM Prompt:\n",
      "\n",
      "Follow these guidelines when describing an image:\n",
      "\n",
      "1.  Use short sentences that are around 6-21 words long. Examples:\n",
      "    -   An unidentified man sits between them.\n",
      "    -   Digital image scanned from black and white negative.\n",
      "\n",
      "2.  Use a 0.27 proportion of stop words in your sentences. Examples:\n",
      "    -   Clipping from Folknik's newsletter about Teatro Campesino.\n",
      "    -   Flyer advertising double feature movie screenings of \"Tarzan the Ape Man\" and the Little Rascals' \"Fly My Kite,\" with free bananas for the first 10 guests at each show\n",
      "\n",
      "3. Use a comma frequency between 0.00 and 0.02. Examples:\n",
      "    -   Mississippi John Hurt (playing guitar) and Bess Lomax Hawes at a Berkeley Folk Music Festival workshop.\n",
      "    -   Bearded man.\n",
      "\n",
      "4. Aim for a passive voice ratio close to 0.03. Examples:\n",
      "    -   Template for form letters to Berkeley Folk Music Festival artists and folklorists\n",
      "    -   Digital image scanned from black and white negative.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the LLM prompt\n",
    "llm_prompt = f\"\"\"\n",
    "Follow these guidelines when describing an image:\n",
    "\n",
    "1.  Use short sentences that are around {lower_bound}-{upper_bound} words long. Examples:\n",
    "    -   {example_sentence_lengths[0] if len(example_sentence_lengths) > 0 else \"No example available\"}\n",
    "    -   {example_sentence_lengths[1] if len(example_sentence_lengths) > 1 else \"No example available\"}\n",
    "\n",
    "2.  Use a {mean_stopword_frequency:.2f} proportion of stop words in your sentences. Examples:\n",
    "    -   {example_stopword_frequencies[0] if len(example_stopword_frequencies) > 0 else \"No example available\"}\n",
    "    -   {example_stopword_frequencies[1] if len(example_stopword_frequencies) > 1 else \"No example available\"}\n",
    "\n",
    "3. Use a comma frequency between {comma_lower_bound:.2f} and {comma_upper_bound:.2f}. Examples:\n",
    "    -   {example_comma_frequencies[0] if len(example_comma_frequencies) > 0 else \"No example available\"}\n",
    "    -   {example_comma_frequencies[1] if len(example_comma_frequencies) > 1 else \"No example available\"}\n",
    "\n",
    "4. Aim for a passive voice ratio close to {mean_passive_voice_ratio:.2f}. Examples:\n",
    "    -   {example_passive_voice_ratios[0] if len(example_passive_voice_ratios) > 0 else \"No example available\"}\n",
    "    -   {example_passive_voice_ratios[1] if len(example_passive_voice_ratios) > 1 else \"No example available\"}\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nLLM Prompt:\")\n",
    "print(llm_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef80fec",
   "metadata": {},
   "source": [
    "Then save the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b6cfe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM prompt has been saved to llm_prompt.txt\n"
     ]
    }
   ],
   "source": [
    "file_path = \"llm_prompt.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(llm_prompt)\n",
    "\n",
    "print(f\"LLM prompt has been saved to {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
